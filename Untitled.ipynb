{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dlib.dlib.ranking_pair object at 0x107c530a0>\n",
      "Ranking score for a relevant vector:     0.5\n",
      "Ranking score for a non-relevant vector: -0.5\n",
      "ranking_accuracy: 1  mean_ap: 1\n",
      "Weights: 0.5\n",
      "-0.5\n",
      "Cross validation results: ranking_accuracy: 1  mean_ap: 1\n",
      "Ranking score for a relevant vector:     0.5\n",
      "Ranking score for a non-relevant vector: -0.5\n"
     ]
    }
   ],
   "source": [
    "# Now let's make some testing data.  To make it really simple, let's suppose\n",
    "# that we are ranking 2D vectors and that vectors with positive values in the\n",
    "# first dimension should rank higher than other vectors.  So what we do is make\n",
    "# examples of relevant (i.e. high ranking) and non-relevant (i.e. low ranking)\n",
    "# vectors and store them into a ranking_pair object like so:\n",
    "data = dlib.ranking_pair()\n",
    "\n",
    "# Here we add two examples.  In real applications, you would want lots of\n",
    "# examples of relevant and non-relevant vectors.\n",
    "data.relevant.append(dlib.vector([1, 0]))\n",
    "data.nonrelevant.append(dlib.vector([0, 1]))\n",
    "\n",
    "# Now that we have some data, we can use a machine learning method to learn a\n",
    "# function that will give high scores to the relevant vectors and low scores to\n",
    "# the non-relevant vectors.\n",
    "trainer = dlib.svm_rank_trainer()\n",
    "# Note that the trainer object has some parameters that control how it behaves.\n",
    "# For example, since this is the SVM-Rank algorithm it has a C parameter that\n",
    "# controls the trade-off between trying to fit the training data exactly or\n",
    "# selecting a \"simpler\" solution which might generalize better. \n",
    "trainer.c = 10\n",
    "\n",
    "# So let's do the training.\n",
    "rank = trainer.train(data)\n",
    "\n",
    "# Now if you call rank on a vector it will output a ranking score.  In\n",
    "# particular, the ranking score for relevant vectors should be larger than the\n",
    "# score for non-relevant vectors.  \n",
    "print(\"Ranking score for a relevant vector:     {}\".format(\n",
    "    rank(data.relevant[0])))\n",
    "print(\"Ranking score for a non-relevant vector: {}\".format(\n",
    "    rank(data.nonrelevant[0])))\n",
    "# The output is the following:\n",
    "#    ranking score for a relevant vector:     0.5\n",
    "#    ranking score for a non-relevant vector: -0.5\n",
    "\n",
    "\n",
    "# If we want an overall measure of ranking accuracy we can compute the ordering\n",
    "# accuracy and mean average precision values by calling test_ranking_function().\n",
    "# In this case, the ordering accuracy tells us how often a non-relevant vector\n",
    "# was ranked ahead of a relevant vector.  In this case, it returns 1 for both\n",
    "# metrics, indicating that the rank function outputs a perfect ranking.\n",
    "print(dlib.test_ranking_function(rank, data))\n",
    "\n",
    "# The ranking scores are computed by taking the dot product between a learned\n",
    "# weight vector and a data vector.  If you want to see the learned weight vector\n",
    "# you can display it like so:\n",
    "print(\"Weights: {}\".format(rank.weights))\n",
    "# In this case the weights are:\n",
    "#  0.5 \n",
    "# -0.5 \n",
    "\n",
    "# In the above example, our data contains just two sets of objects.  The\n",
    "# relevant set and non-relevant set.  The trainer is attempting to find a\n",
    "# ranking function that gives every relevant vector a higher score than every\n",
    "# non-relevant vector.  Sometimes what you want to do is a little more complex\n",
    "# than this. \n",
    "#\n",
    "# For example, in the web page ranking example we have to rank pages based on a\n",
    "# user's query.  In this case, each query will have its own set of relevant and\n",
    "# non-relevant documents.  What might be relevant to one query may well be\n",
    "# non-relevant to another.  So in this case we don't have a single global set of\n",
    "# relevant web pages and another set of non-relevant web pages.  \n",
    "#\n",
    "# To handle cases like this, we can simply give multiple ranking_pair instances\n",
    "# to the trainer.  Therefore, each ranking_pair would represent the\n",
    "# relevant/non-relevant sets for a particular query.  An example is shown below\n",
    "# (for simplicity, we reuse our data from above to make 4 identical \"queries\").\n",
    "queries = dlib.ranking_pairs()\n",
    "queries.append(data)\n",
    "queries.append(data)\n",
    "queries.append(data)\n",
    "queries.append(data)\n",
    "\n",
    "# We can train just as before.  \n",
    "rank = trainer.train(queries)\n",
    "\n",
    "# Now that we have multiple ranking_pair instances, we can also use\n",
    "# cross_validate_ranking_trainer().  This performs cross-validation by splitting\n",
    "# the queries up into folds.  That is, it lets the trainer train on a subset of\n",
    "# ranking_pair instances and tests on the rest.  It does this over 4 different\n",
    "# splits and returns the overall ranking accuracy based on the held out data.\n",
    "# Just like test_ranking_function(), it reports both the ordering accuracy and\n",
    "# mean average precision.\n",
    "print(\"Cross validation results: {}\".format(\n",
    "    dlib.cross_validate_ranking_trainer(trainer, queries, 4)))\n",
    "\n",
    "# Finally, note that the ranking tools also support the use of sparse vectors in\n",
    "# addition to dense vectors (which we used above).  So if we wanted to do\n",
    "# exactly what we did in the first part of the example program above but using\n",
    "# sparse vectors we would do it like so:\n",
    "\n",
    "data = dlib.sparse_ranking_pair()\n",
    "samp = dlib.sparse_vector()\n",
    "\n",
    "# Make samp represent the same vector as dlib.vector([1, 0]).  In dlib, a sparse\n",
    "# vector is just an array of pair objects.  Each pair stores an index and a\n",
    "# value.  Moreover, the svm-ranking tools require sparse vectors to be sorted\n",
    "# and to have unique indices.  This means that the indices are listed in\n",
    "# increasing order and no index value shows up more than once.  If necessary,\n",
    "# you can use the dlib.make_sparse_vector() routine to make a sparse vector\n",
    "# object properly sorted and contain unique indices. \n",
    "samp.append(dlib.pair(0, 1))\n",
    "data.relevant.append(samp)\n",
    "\n",
    "# Now make samp represent the same vector as dlib.vector([0, 1])\n",
    "samp.clear()\n",
    "samp.append(dlib.pair(1, 1))\n",
    "data.nonrelevant.append(samp)\n",
    "\n",
    "trainer = dlib.svm_rank_trainer_sparse()\n",
    "rank = trainer.train(data)\n",
    "print(\"Ranking score for a relevant vector:     {}\".format(\n",
    "    rank(data.relevant[0])))\n",
    "print(\"Ranking score for a non-relevant vector: {}\".format(\n",
    "    rank(data.nonrelevant[0])))\n",
    "# Just as before, the output is the following:\n",
    "#    ranking score for a relevant vector:     0.5\n",
    "#    ranking score for a non-relevant vector: -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
